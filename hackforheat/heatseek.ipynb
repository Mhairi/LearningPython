{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sensorcsv = open(\"C:/Users/dfthd/Desktop/heatseek_readings.csv\")\n",
    "sensordata = pd.read_csv(sensorcsv)\n",
    "\n",
    "usercsv = open(\"C:/Users/dfthd/Desktop/heatseek_users.csv\")\n",
    "userdata = pd.read_csv(usercsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 98,  25,   3,  87, 113, 114, 130, 115, 119,  26,   1,   5, 172,\n",
       "       126, 127, 132, 134, 136, 137, 138, 135,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  66,  67,  68,  69,  70,  71,  72,\n",
       "        73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,\n",
       "       150, 124, 125, 123, 147, 145, 144, 143, 146,   7,  11,  12,  13,\n",
       "        14, 154, 155, 159, 158,  23, 160, 161, 163, 162, 165,  89,  92,\n",
       "        91,  90, 164,  88, 167, 166, 168, 169, 170, 112], dtype=int64)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#sensordata.user_id.unique()\n",
    "#sensordata.user_id.value_counts()\n",
    "#sensordata.violation.value_counts() #This returns the number of 't's and 'f's\n",
    "#np.sort(userdata.id.unique())\n",
    "#np.intersect1d(userdata.id.unique(), sensordata.user_id.unique()) #Returns the common ids in both the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Triage\n",
    "#We want to have a measure of which users are facing the most chronic problems.\n",
    "\n",
    "#Write some code that subsets all the violation == 't' cases\n",
    "sensordataviolations = sensordata[sensordata.violation == 't'] #here it is.\n",
    "\n",
    "#Hackiest method: just number of violations/numberof nonviolations and sort users by that\n",
    "#That is, which users have had the most violations given the total number of readings \n",
    "\n",
    "violationsovertime = []\n",
    "\n",
    "for i in sensordata.user_id.unique():\n",
    "    nonviolations = sensordata.loc[sensordata.user_id == i, 'violation'].value_counts()['f'] #Number of violations = 'f'\n",
    "    try:\n",
    "        violations = sensordata.loc[sensordata.user_id == i, 'violation'].value_counts()['t'] #Number of violations = 't'\n",
    "    except KeyError:\n",
    "        violations = 0    \n",
    "    sensordata.loc[sensordata.user_id == i, 'vfreq'] = float(violations)/float(nonviolations)\n",
    "    violationsovertime.append([i, (float(violations)/float(nonviolations))])\n",
    "\n",
    "#violations over time gives first the user_id, then the proportion of how many of their readings are violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 0 days 00:00:00.011915\n",
      "25 0 days 00:48:36.900000\n",
      "3 0 days 06:04:00.262110\n",
      "87 0 days 13:55:44.925653\n",
      "113 0 days 00:59:59.972928\n",
      "114 0 days 01:01:14.343489\n",
      "130 0 days 00:53:36.506154\n",
      "115 0 days 01:00:59.473623\n",
      "119 0 days 01:00:36.331119\n",
      "26 0 days 00:00:00.000031\n",
      "1 0 days 00:46:15.231805\n",
      "5 0 days 00:49:50.485335\n",
      "172 0 days 00:19:12.928003\n",
      "126 0 days 00:06:28.098409\n",
      "127 0 days 01:01:35.454545\n",
      "132 0 days 00:43:52.864636\n",
      "134 0 days 02:12:07.492227\n",
      "136 0 days 00:03:49.787234\n",
      "137 0 days 01:00:58.096774\n",
      "138 0 days 00:27:27.259259\n",
      "135 0 days 00:58:54.997742\n",
      "34 0 days 01:00:00\n",
      "35 0 days 01:00:00\n",
      "36 0 days 01:00:00\n",
      "37 0 days 01:00:00\n",
      "38 0 days 01:00:00\n",
      "39 0 days 01:00:00\n",
      "40 0 days 01:00:00\n",
      "41 0 days 01:00:00\n",
      "42 0 days 01:00:00\n",
      "43 0 days 01:00:00\n",
      "44 0 days 01:00:00\n",
      "45 0 days 01:00:00\n",
      "46 0 days 01:00:00\n",
      "47 0 days 01:00:00\n",
      "48 0 days 01:00:00\n",
      "49 0 days 01:00:00\n",
      "50 0 days 01:00:00\n",
      "51 0 days 01:00:00\n",
      "52 0 days 01:00:00\n",
      "53 0 days 01:00:00\n",
      "54 0 days 01:00:00\n",
      "55 0 days 01:00:00\n",
      "56 0 days 01:00:00\n",
      "57 0 days 01:00:00\n",
      "66 0 days 01:00:00\n",
      "67 0 days 01:00:00\n",
      "68 0 days 01:00:00\n",
      "69 0 days 01:00:00\n",
      "70 0 days 01:00:00\n",
      "71 0 days 01:00:00\n",
      "72 0 days 01:00:00\n",
      "73 0 days 01:00:00\n",
      "74 0 days 01:00:00\n",
      "75 0 days 01:00:00\n",
      "76 0 days 01:00:00\n",
      "77 0 days 01:00:00\n",
      "78 0 days 01:00:00\n",
      "79 0 days 01:00:00\n",
      "80 0 days 01:00:00\n",
      "81 0 days 01:00:00\n",
      "82 0 days 01:00:00\n",
      "83 0 days 01:00:00\n",
      "84 0 days 01:00:00\n",
      "85 0 days 01:00:00\n",
      "150 0 days 00:00:11.899668\n",
      "124 0 days 00:44:07.800000\n",
      "125 0 days 00:00:06.250000\n",
      "123 0 days 00:31:33.348914\n",
      "147 0 days 00:27:06.601328\n",
      "145 0 days 00:26:38.722852\n",
      "144 0 days 00:31:26.399232\n",
      "143 0 days 00:19:15.622911\n",
      "146 0 days 00:26:16.490196\n",
      "7 7 Too few data points?\n",
      "11 0 days 01:00:00\n",
      "12 0 days 01:00:00\n",
      "13 0 days 01:00:00\n",
      "14 0 days 01:00:00\n",
      "154 0 days 01:33:02.114751\n",
      "155 0 days 00:00:14.351941\n",
      "159 0 days 01:02:33.195668\n",
      "158 0 days 01:02:36.827371\n",
      "23 0 days 00:00:00.721776\n",
      "160 0 days 01:12:58.020338\n",
      "161 0 days 00:52:34.203125\n",
      "163 0 days 00:31:07.745874\n",
      "162 0 days 00:55:33.859016\n",
      "165 0 days 00:57:50.399317\n",
      "89 89 Too few data points?\n",
      "92 0 days 07:55:59.244171\n",
      "91 5 days 09:45:30.543584\n",
      "90 0 days 07:55:58.570181\n",
      "164 0 days 01:00:07.381818\n",
      "88 0 days 01:02:18.576923\n",
      "167 167 Too few data points?\n",
      "166 0 days 01:10:12.485714\n",
      "168 0 days 01:01:15.446913\n",
      "169 0 days 12:30:04\n",
      "170 0 days 01:00:00.034482\n",
      "112 0 days 01:01:56.177419\n"
     ]
    }
   ],
   "source": [
    "#Data cleaning\n",
    "\n",
    "#Getting rid of test cases:\n",
    "#1. Can we just delete by test IDs?\n",
    "#2. If testing was separated by a minute, we can find all test cases by looping through all users,\n",
    "#  and if they have a bunch of data that was collected within minutes, delete the user?\n",
    "\n",
    "#We first convert the string dates into datetime format\n",
    "sensordata['formatteddate'] = sensordata.created_at.apply(lambda x: pd.to_datetime(x,  format = \"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "#Then, one way of telling if a user_id was an actual user or a test case was to calculate the average timedelta for each user_id.\n",
    "#Timedeltas of 1min are tests, 1 hour are users (don't know if this is always true, but if no user has an average polling rate\n",
    "#of 1 min, we can use a bunch of methods to filter away test cases step by step).\n",
    "\n",
    "sensordata['averagetimedelta'] = 0.00 #makes a new column\n",
    "\n",
    "for i in sensordata.user_id.unique(): #for each user\n",
    "    timelist = sensordata.loc[sensordata.user_id == i, 'formatteddate'] #this gives us a list of all their times in timestamp\n",
    "    timedeltas = [] \n",
    "    for j in range(1, len(timelist)-1):\n",
    "        timedeltas.append(timelist.iloc[j] - timelist.iloc[j-1]) #list of differences in time between time point j and j-1\n",
    "    try:\n",
    "        #we print the user_id, followd by their average time delta from point j to j-1\n",
    "        print i, abs(sum(timedeltas, datetime.timedelta(0)))/len(timedeltas) #the average timedelta\n",
    "    except ZeroDivisionError: #some cases have too few points (and results in a zero division error)\n",
    "        #instead of breaking with we encounter a zerodivisionerror, just print the following:\n",
    "        print i, \"Too few data points?\"\n",
    "    sensordata.loc[sensordata.user_id == i, 'averagetimedelta'] = averagetimedeltas.total_seconds()\n",
    "\n",
    "#3. If user ids are recycled, we'll have to do a combination of those things.\n",
    "\n",
    "#Some sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stuff to do for fun\n",
    "\n",
    "#Variability/consistency\n",
    "#Which buildings have the least/most variable temperatures?\n",
    "#For this, we just calculate within-person variability (how much do sensor temperatures by the same user) vary as a function of time\n",
    "#We an use this same process to calculate variability between locations (e.g., just calculate variance for each location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now, we loop over all unique users in the dataset and generate a measure of how long they've had the sensor running\n",
    "sensordata['totaltime'] = 0\n",
    "sensordata['vfreq'] = 0\n",
    "\n",
    "for i in sensordata.user_id.unique():\n",
    "    firstentry = len(sensordata.loc[sensordata.user_id==i,'formatteddate']) #This gives us the index of the first timepoint\n",
    "    lasttime = sensordata.loc[sensordata.user_id == i, 'formatteddate'].iloc[0] #This was the timestamp of the latest timepoint\n",
    "    firsttime =  sensordata.loc[sensordata.user_id == i, 'formatteddate'].iloc[firstentry-1] #This was the timestamp of the first timepoint\n",
    "    sensordata.loc[sensordata.user_id == i, 'totaltime'] = lasttime - firsttime #This is the timedelta (over how long a period readings were made)\n",
    "    #print i, lasttime-firsttime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
